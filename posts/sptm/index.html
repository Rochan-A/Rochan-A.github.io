<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Rochan Avlur Venkat">
    <meta name="description" content="Personal website of Rochan Avlur">
    <meta name="keywords" content="rochan, rochan avlur, Rochan Avlur, Rochan Avlur Venkat, blog, developer, personal">

    <base href="https://rochan-a.github.io/">
    <title>
  The journey: sptm - Sentence topic Prediction using Topic Modeling · Rochan Avlur
</title>

    <link rel="canonical" href="https://rochan-a.github.io/posts/sptm/">

    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="//cdn.rawgit.com/necolas/normalize.css/master/normalize.css">
    <link rel="stylesheet" href="https://rochan-a.github.io/css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="https://rochan-a.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://rochan-a.github.io/images/favicon-16x16.png" sizes="16x16">

    

    <meta name="generator" content="Hugo 0.42.2" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://rochan-a.github.io/">
      Rochan Avlur
    </a>
    
    <ul class="navigation-list  float-right ">
      
      <li class="navigation-item">
        <a class="navigation-link" href="essays/">Essays</a>
      </li>
      
      <li class="navigation-item">
        <a class="navigation-link" href="posts/">Blog</a>
      </li>
      
      <li class="navigation-item">
        <a class="navigation-link" href="https://drive.google.com/file/d/0B3-4HSlT5TS-YUc1MlgtVEdKZUVKTGRJRjlialpjci1PSUg4/view?usp=sharing">Resume</a>
      </li>
      
      <li class="navigation-item">
        <a class="navigation-link" href="about/">About</a>
      </li>
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
  <article>
    <header>
      <h1 class="title">The journey: sptm - Sentence topic Prediction using Topic Modeling</h1>
      <h2 class="date">July 6, 2018</h2>

      
    </header>

    

<p>During my summer break of freshman year at college, I got the opportunity to work as an intern alongside <a href="https://www.linkedin.com/in/anupam-mediratta-60b5547/">Anupam Meridatta</a>. My internship required me to work on a Topic Prediction model that later had to be incorporated in a bigger project.</p>

<p>To begin with, let us take a quick look at the current landscape of the industry, with a focus on prediction models and its applications.</p>

<h2 id="prediction-models">Prediction models</h2>

<p>Simply put, prediction makes our lives easy. It reduces the effort from our side making the experience of whatever we are doing much more enjoyable and easier. Prediction models have become an area of intense research and innovation, both from a mathematical stand point and an application level. Today, we can find prediction models being used everywhere around us; from when we wake up and check our email to sleep pattern prediction to name a few examples.</p>

<p>Complexity of prediction models vary from application to application. Performance, accuracy, model size are a few examples of parameters based on which models are evaluated. One topic of great research and importance is building prediction models for Natural Language Processing (NLP) applications. The task of training and achieving high accuracies from NLP prediction models can make or break an application. Communication is one of the most fundamental aspects of humans existence and the mere act of making sense of inputted text can be a great challenge, let alone, predict what will be spoken in the near future.</p>

<h2 id="hotel-reviews">Hotel Reviews</h2>

<p>The end application for the topic prediction model that I worked during the duration of the internship dealt with hotel review. Analysis of hotel reviews helps in understanding customers opinion and suggestions. Hotel management use this information to improve their services etc. If models can be trained to analyse this review data and carry out the required action, it will be invaluable to Hotels.</p>

<h2 id="topic-modeling">Topic Modeling</h2>

<p>Topic modeling is the process of training models on a large corpus (collection of documents) of data to identify different topics present in the corpus. Some example applications of topic models are sorting documents, identification etc. You can read more about topics models <a href="https://wikipidia.org/topic_modeling">here</a>.</p>

<p>The deliverable of the internship was a Topic Prediction Model. The model was supposed to be able to predict the topic of the next sentence / document.</p>

<h2 id="the-journey">The Journey</h2>

<p>My internship under Anupam was for 2 months. During the internship, I came across many different frameworks and libraries. The next sections, I have tried to go into detail, explaining each step and decision that I took during the internship.</p>

<h3 id="opinirank-dataset">OpiniRank Dataset</h3>

<p>For training models, I used the <a href="http://kavita-ganesan.com/entity-ranking-data/#.W7Da7BThVhE">OpiniRank Dataset</a>. This dataset contains reviews from Tripadvisor, approximately 259,000 reviews. From the dataset description:
 - Full reviews of hotels in 10 different cities (Dubai, Beijing, London, New York City, New Delhi, San Francisco, Shanghai, Montreal, Las Vegas, Chicago)
 - There are about 80-700 hotels in each city
 - Extracted fields include date, review title and the full review
 - Total number of reviews: ~259,000</p>

<p>There should be 10 different folders representing the 10 cities mentioned earlier. Each file (within these 10 folders) would contain all reviews related to a particular hotel. The filename represents the name of the hotel. Within each file, you would see a set of reviews in the following format:</p>
<div class="highlight"><pre style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4">Date1&lt;tab&gt;Review title1&lt;tab&gt;Full review 1
Date2&lt;tab&gt;Review title2&lt;tab&gt;Full review 2
...
...</pre></div>
<p>The dataset was chosen as it was easy to parse and contained reviews from multiple cities. This increased the variation in the reviews making the model more general.</p>

<h3 id="word2vec-fasttext-pre-trained-models">Word2vec , FastText &amp; pre-trained models</h3>

<p>Word2vec is a well known concept, used to generate representation vectors out of words.</p>

<p>In general, when you like to build some model using words, simply labeling/one-hot encoding them is a plausible way to go. However, when using such encoding, the words lose their meaning.</p>

<p>The word2vec, presented in 2013, intends to give you just that: a numeric representation for each word, that will be able to capture such relations as above. this is part of a wider concept in machine learning — the feature vectors.</p>

<p>Such representations, encapsulate different relations between words, like synonyms, antonyms, or analogies. Word2vec representation is created using 2 algorithms: Continuous Bag-of-Words model (CBOW) and the Skip-Gram model.</p>

<p>The initial approach to train topic models was to use text classification techniques. FastText, from Facebook AI Research Lab provides pre-trained english word vectors.</p>

<blockquote>
<p>FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices.</p>
</blockquote>

<p>More information can be found <a href="https://fasttext.cc/">here</a>.</p>

<p>Initially, the plan was to use word embedding and from the ground up build a custom topic modeling algorithm. However, to achieve the deliverables, I decided not to go ahead with this line of action as implementing it in such a short time would have been a challenge.</p>

<h3 id="doc2vec">Doc2vec</h3>

<p>Similar to word embedding, doc2vec does the same thing but to documents. The goal of doc2vec is to create a numeric representation of a document, regardless of its length. But unlike words, documents do not come in logical structures such as words, so the another method has to be found.</p>

<p>This can be done by using the word2vec model, and adding another vector (Paragraph ID). So, instead of using just words to predict the next word, we also added another feature vector, which is document-unique.</p>

<p>That said, when training the word vectors W, the document vector D is trained as well, and in the end of training, it holds a numeric representation of the document.</p>

<p>The model above is called Distributed Memory version of Paragraph Vector (PV-DM). It acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document.</p>

<p>The results from doc2vec did not meet the minimum accuracy that I needed. This can be attributed to the complexity and variety of hotel reviews.</p>

<h3 id="gensim-lda-vs-lsa">Gensim &amp; LDA vs. LSA</h3>

<p>While working on doc2vec, I came across Gensim:</p>

<blockquote>
<p>Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.</p>
</blockquote>

<p>More information can be found <a href="https://radimrehurek.com/gensim/">here</a>.</p>

<p>The Gensim toolkit comes with everything one will need for topic modeling. Personally, I found it easy to build prototypes with various models, extend it with additional features and gain empirical insights quickly. It&rsquo;s a reliable library that can be used beyond prototyping too.</p>

<p>I decided to fall back to Latent Dirichlet allocation (LDA) and Latent Semantic Analysis (LSA) after the result from doc2vec. Gensim classes, specifically <i>ldamodel</i> were used here. The results from the testing rounds convinced us enough to decide to use LDA.</p>

<h3 id="lda2vec">Lda2vec</h3>

<p>I would like to mention one more library / project that caught my interest. As explained on its <a href="https://github.com/cemoody/lda2vec">GitHub</a> page:</p>

<blockquote>
<p>The lda2vec model tries to mix the best parts of word2vec and LDA into a single framework. word2vec captures powerful relationships between words, but the resulting vectors are largely un-interpretable and don&rsquo;t represent documents. LDA on the other hand is quite interpretable by humans, but doesn&rsquo;t model local word relationships like word2vec. We build a model that builds both word and document topics, makes them interpretable, makes topics over clients, times, and documents, and makes them supervised topics.</p>
</blockquote>

<p>As much as I would have loved to try lda2vec, at the time, I couldn&rsquo;t get it working and it had been over 2 years since the last update.</p>

<p>I might try to work on forking lda2vec and make an updated version of it. I will post any updates on it.</p>

<h3 id="gensim-mallet">Gensim + MALLET</h3>

<p>Training LDA models using Gensim was time consuming. While looking for a more optimized framework, I came across <a href="http://mallet.cs.umass.edu/">MALLET</a>:</p>

<blockquote>
<p>MALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.</p>
</blockquote>

<p>Mallet provides optimized topic modeling features:</p>

<blockquote>
<p>Topic models are useful for analyzing large collections of unlabeled text. The MALLET topic modeling toolkit contains efficient, sampling-based implementations of Latent Dirichlet Allocation, Pachinko Allocation, and Hierarchical LDA.</p>
</blockquote>

<p>The next step was to link Gensim and MALLET together. For this, Gensim&rsquo;s <i>wrapper.ldamallet</i> was used.</p>

<h3 id="conditional-probability">Conditional Probability</h3>

<p>Finally, after training the model, the conditional probability for each topic versus topic was calculated.</p>

  </article>

  <br/>

  
  
</section>

      </div>

      <footer class="footer">
  <section class="container">
    
     © 2018    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>. 
  </section>
</footer>

    </main>

    

  </body>

</html>
